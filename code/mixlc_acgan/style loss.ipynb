{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b8e37f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from dataset import SatelliteDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72006e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_loss_fn(phi1, phi2):\n",
    "    batch_size, c, h, w = phi1.shape\n",
    "    psi1 = phi1.reshape((batch_size, c, w*h))\n",
    "    psi2 = phi2.reshape((batch_size, c, w*h))\n",
    "    \n",
    "    gram1 = torch.matmul(psi1, torch.transpose(psi1, 1, 2)) / (c*h*w)\n",
    "    gram2 = torch.matmul(psi2, torch.transpose(psi2, 1, 2)) / (c*h*w)\n",
    "    # as described in johnson et al.\n",
    "    return torch.sum(torch.norm(gram1 - gram2, p = \"fro\", dim=(1,2))) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03e11f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\hugih/.cache\\torch\\hub\\pytorch_vision_v0.9.0\n"
     ]
    }
   ],
   "source": [
    "vgg_model = torch.hub.load('pytorch/vision:v0.9.0', 'vgg16', pretrained=True)\n",
    "relu3_3 = torch.nn.Sequential(*vgg_model.features[:16])\n",
    "relu4_3 = torch.nn.Sequential(*vgg_model.features[:23])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a77cf426",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize =  transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                                     std=[0.5, 0.5, 0.5])\n",
    "toTensor = transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "914ff1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../../data/train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da1c1465",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_files = os.listdir(os.path.join(data_dir, \"rgb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fb93149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img(idx):\n",
    "    img_fn = os.path.join(data_dir, \"rgb\", rgb_files[idx])\n",
    "    with Image.open(img_fn) as img:\n",
    "        img = toTensor(img)[:3,:,:]\n",
    "        img = normalize(img)\n",
    "    img = img.reshape((1, 3, 256, 256))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36d6a7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = get_img(10)\n",
    "img2 = get_img(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eddaf0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "print(torch.max(img1))\n",
    "print(torch.max(img2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bdf7952d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.6706, -0.6863, -0.6863,  ..., -0.8588, -0.8745, -0.8353],\n",
       "          [-0.6863, -0.6941, -0.7020,  ..., -0.8980, -0.8824, -0.8745],\n",
       "          [-0.6784, -0.6784, -0.6863,  ..., -0.8588, -0.8588, -0.8667],\n",
       "          ...,\n",
       "          [-0.7255, -0.7333, -0.7176,  ..., -0.8824, -0.8824, -0.8824],\n",
       "          [-0.7412, -0.7255, -0.7098,  ..., -0.8824, -0.8824, -0.8745],\n",
       "          [-0.7569, -0.7333, -0.7333,  ..., -0.8824, -0.8824, -0.8902]],\n",
       "\n",
       "         [[-0.5451, -0.5373, -0.5608,  ..., -0.6392, -0.6157, -0.6157],\n",
       "          [-0.5373, -0.5529, -0.5529,  ..., -0.7098, -0.7020, -0.6863],\n",
       "          [-0.5373, -0.5451, -0.5373,  ..., -0.6627, -0.6706, -0.6627],\n",
       "          ...,\n",
       "          [-0.6157, -0.6078, -0.6078,  ..., -0.7569, -0.7569, -0.7647],\n",
       "          [-0.6392, -0.6314, -0.6078,  ..., -0.7804, -0.8039, -0.7490],\n",
       "          [-0.6314, -0.6392, -0.5843,  ..., -0.7725, -0.7647, -0.7725]],\n",
       "\n",
       "         [[-0.7412, -0.7412, -0.7176,  ..., -0.8980, -0.8902, -0.8667],\n",
       "          [-0.7176, -0.7412, -0.7333,  ..., -0.8980, -0.9059, -0.8980],\n",
       "          [-0.7176, -0.7412, -0.7412,  ..., -0.8824, -0.8824, -0.8902],\n",
       "          ...,\n",
       "          [-0.7647, -0.7725, -0.7882,  ..., -0.8824, -0.9059, -0.8902],\n",
       "          [-0.7804, -0.7804, -0.7961,  ..., -0.9216, -0.9294, -0.8902],\n",
       "          [-0.8039, -0.7961, -0.7882,  ..., -0.9294, -0.9294, -0.9137]]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fe27019",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi1 = relu3_3(img1)\n",
    "phi2 = relu3_3(img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7d01f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "302d8d2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4954, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "style_loss_fn(phi1, phi2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "760bc2a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.3471, grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse(phi1, phi2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf93cf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = SatelliteDataset(\"../../data/train\")\n",
    "loader = DataLoader(ds, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cf4647e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from generator import Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e3807232",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6150dcf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "style loss tensor(17.8600, grad_fn=<SumBackward0>)\n",
      "mse loss tensor(8.4848, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "for rgb_a, rgb_b, rgb_ab, lc_a, lc_b, lc_b_mask, lc_ab, masked_areas in loader:\n",
    "    \n",
    "    fake_img = gen(rgb_a, lc_a, lc_b_mask)\n",
    "    feature_ab = relu3_3(rgb_ab)\n",
    "    feature_fake = relu3_3(fake_img)\n",
    "    \n",
    "    st_loss = style_loss_fn(feature_ab, feature_fake)\n",
    "    print(\"style loss\",st_loss)\n",
    "    print(\"mse loss\", mse(feature_ab, feature_fake))\n",
    "    st_loss.backward()\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e0fcab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
